// This file is generated by ./jumpNdivsteps.py


#include <immintrin.h>
#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
#include <string.h>
#include "gf4591_avx2.h"
#include "polymul_NxN.h"
#include "polymul_32x32.h"
#include "jumpdivsteps.h"


int jump64divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr);
int jump64xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int x);
extern int jump32divsteps(int delta, __m256i *f, __m256i *g, __m256i *buf);
extern int jump32xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *buf, int x);

int jump64divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr){

  __m256i buf[24], ff[4], gg[4];
  __m256i mask, mask1, temp;

  delta = jump32divsteps(delta, f, g, buf);

  // [ff[0, 3]] = [buf[ 0, 1] [ 2, 3]] [f[ 2, 3]] + [buf[ 8, 9]]
  // [gg[0, 3]]   [buf[ 4, 5] [ 6, 7]] [g[ 2, 3]]   [buf[10,11]]

  gf_polymul_32x32_avx2_uvqr_fg(ff,gg,buf,f+2,g+2);
  ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[8]));
  ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[9]));
  gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[10]));
  gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[11]));

  delta = jump32divsteps(delta, ff, gg, buf+12);

  // [uvqr[16,19]] = [buf[12,13] [14,15]] [ff[ 2, 3]] + [buf[20,21]]
  // [uvqr[20,23]]   [buf[16,17] [18,19]] [gg[ 2, 3]]   [buf[22,23]]
  gf_polymul_32x32_avx2_uvqr_fg(uvqr+16,uvqr+20,buf+12,ff+2,gg+2);

  uvqr[16] = barrett_fake(_mm256_add_epi16(uvqr[16],buf[20]));
  uvqr[17] = barrett_fake(_mm256_add_epi16(uvqr[17],buf[21]));
  uvqr[20] = barrett_fake(_mm256_add_epi16(uvqr[20],buf[22]));
  uvqr[21] = barrett_fake(_mm256_add_epi16(uvqr[21],buf[23]));

  // [uvqr[ 0, 3]  [ 4, 7]] = [buf[12,13] [14,15]] [buf[ 0, 1] [ 2, 3]]
  // [uvqr[ 8,11]  [12,15]] = [buf[16,17] [18,19]] [buf[ 4, 5] [ 6, 7]]

  gf_polymul_32x32_avx2_uvqr_vr(uvqr,uvqr+8,buf+12,buf,buf+4);
  gf_polymul_32x32_avx2_uvqr_vr(uvqr+4,uvqr+12,buf+12,buf+2,buf+6);

  return(delta);
}


int jump64xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int t){

  __m256i buf[24], ff[4], gg[4];
  __m256i fx[8], gx[8];
  __m256i mask,mask1,temp;
  int i;

  memset(fx,0,sizeof(fx)); memset(gx,0,sizeof(gx));

  if (t <= 32){
    delta = jump32xdivsteps(delta, f, g, buf, t);

    // [ff[0, 3]]=x^( 32-t) [buf[ 0, 1] [ 2, 3]] [f[ 2, 3]] + [buf[ 8, 9]]
    // [gg[0, 3]]           [buf[ 4, 5] [ 6, 7]] [g[ 2, 3]]   [buf[10,11]]

    gf_polymul_32x32_avx2_uvqr_fg(ff,gg,buf,f+2,g+2);
    memcpy((short *)fx+(32-t),ff,128);
    memcpy((short *)gx+(32-t),gg,128);
    memset(uvqr,0,768);
    memcpy(uvqr,buf,64);
    memcpy(uvqr+4,buf+2,64);
    memcpy(uvqr+8,buf+4,64);
    memcpy(uvqr+12,buf+6,64);
    uvqr[16] = barrett_fake(_mm256_add_epi16(fx[0],buf[8]));
    uvqr[17] = barrett_fake(_mm256_add_epi16(fx[1],buf[9]));
    uvqr[18] = fx[2];
    uvqr[19] = fx[3];
    uvqr[20] = barrett_fake(_mm256_add_epi16(gx[0],buf[10]));
    uvqr[21] = barrett_fake(_mm256_add_epi16(gx[1],buf[11]));
    uvqr[22] = gx[2];
    uvqr[23] = gx[3];
  } else {
    delta = jump32divsteps(delta, f, g, buf);

    // [ff[0, 3]] = [buf[ 0, 1] [ 2, 3]] [f[ 2, 3]] + [buf[ 8, 9]]
    // [gg[0, 3]]   [buf[ 4, 5] [ 6, 7]] [g[ 2, 3]]   [buf[10,11]]

    gf_polymul_32x32_avx2_uvqr_fg(ff,gg,buf,f+2,g+2);
    ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[8]));
    ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[9]));
    gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[10]));
    gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[11]));

    delta = jump32xdivsteps(delta, ff, gg, buf+12, t-32);

    // [uvqr[16,19]]=x^( 64-t)[buf[12,13] [14,15]] [ff[ 2, 3]] + [buf[20,21]]
    // [uvqr[20,23]]          [buf[16,17] [18,19]] [gg[ 2, 3]]   [buf[22,23]]

    gf_polymul_32x32_avx2_uvqr_fg_x(uvqr+16,uvqr+20,buf+12,ff+2,gg+2);
    memcpy((short *)fx+(64-t),uvqr+16,128);
    memcpy((short *)gx+(64-t),uvqr+20,128);
    uvqr[16] = barrett_fake(_mm256_add_epi16(fx[0],buf[20]));
    uvqr[17] = barrett_fake(_mm256_add_epi16(fx[1],buf[21]));
    uvqr[18] = fx[2];
    uvqr[19] = fx[3];
    uvqr[20] = barrett_fake(_mm256_add_epi16(gx[0],buf[22]));
    uvqr[21] = barrett_fake(_mm256_add_epi16(gx[1],buf[23]));
    uvqr[22] = gx[2];
    uvqr[23] = gx[3];

    // [uvqr[ 0, 3]  [ 4, 7]] = [buf[12,13] [14,15]] [buf[ 0, 1] [ 2, 3]]
    // [uvqr[ 8,11]  [12,15]] = [buf[16,17] [18,19]] [buf[ 4, 5] [ 6, 7]]

    gf_polymul_32x32_avx2_uvqr_vr_x(uvqr,uvqr+8,buf+12,buf,buf+4);
    gf_polymul_32x32_avx2_uvqr_vr_x(uvqr+4,uvqr+12,buf+12,buf+2,buf+6);
  }
  return(delta);
}
int jump128divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr);
int jump128xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int x);
extern int jump64divsteps(int delta, __m256i *f, __m256i *g, __m256i *buf);
extern int jump64xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *buf, int x);

int jump128divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr){

  __m256i buf[48], ff[8], gg[8];
  __m256i mask, mask1, temp;

  delta = jump64divsteps(delta, f, g, buf);

  // [ff[0, 7]] = [buf[ 0, 3] [ 4, 7]] [f[ 4, 7]] + [buf[16,19]]
  // [gg[0, 7]]   [buf[ 8,11] [12,15]] [g[ 4, 7]]   [buf[20,23]]

  gf_polymul_64x64_avx2_uvqr_fg(ff,gg,buf,f+4,g+4);
  ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[16]));
  ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[17]));
  ff[2] = barrett_fake(_mm256_add_epi16(ff[2],buf[18]));
  ff[3] = barrett_fake(_mm256_add_epi16(ff[3],buf[19]));
  gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[20]));
  gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[21]));
  gg[2] = barrett_fake(_mm256_add_epi16(gg[2],buf[22]));
  gg[3] = barrett_fake(_mm256_add_epi16(gg[3],buf[23]));

  delta = jump64divsteps(delta, ff, gg, buf+24);

  // [uvqr[32,39]] = [buf[24,27] [28,31]] [ff[ 4, 7]] + [buf[40,43]]
  // [uvqr[40,47]]   [buf[32,35] [36,39]] [gg[ 4, 7]]   [buf[44,47]]
  gf_polymul_64x64_avx2_uvqr_fg(uvqr+32,uvqr+40,buf+24,ff+4,gg+4);

  uvqr[32] = barrett_fake(_mm256_add_epi16(uvqr[32],buf[40]));
  uvqr[33] = barrett_fake(_mm256_add_epi16(uvqr[33],buf[41]));
  uvqr[34] = barrett_fake(_mm256_add_epi16(uvqr[34],buf[42]));
  uvqr[35] = barrett_fake(_mm256_add_epi16(uvqr[35],buf[43]));
  uvqr[40] = barrett_fake(_mm256_add_epi16(uvqr[40],buf[44]));
  uvqr[41] = barrett_fake(_mm256_add_epi16(uvqr[41],buf[45]));
  uvqr[42] = barrett_fake(_mm256_add_epi16(uvqr[42],buf[46]));
  uvqr[43] = barrett_fake(_mm256_add_epi16(uvqr[43],buf[47]));

  // [uvqr[ 0, 7]  [ 8,15]] = [buf[24,27] [28,31]] [buf[ 0, 3] [ 4, 7]]
  // [uvqr[16,23]  [24,31]] = [buf[32,35] [36,39]] [buf[ 8,11] [12,15]]

  gf_polymul_64x64_avx2_uvqr_vr(uvqr,uvqr+16,buf+24,buf,buf+8);
  gf_polymul_64x64_avx2_uvqr_vr(uvqr+8,uvqr+24,buf+24,buf+4,buf+12);

  return(delta);
}


int jump128xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int t){

  __m256i buf[48], ff[8], gg[8];
  __m256i fx[16], gx[16];
  __m256i mask,mask1,temp;
  int i;

  memset(fx,0,sizeof(fx)); memset(gx,0,sizeof(gx));

  if (t <= 64){
    delta = jump64xdivsteps(delta, f, g, buf, t);

    // [ff[0, 7]]=x^( 64-t) [buf[ 0, 3] [ 4, 7]] [f[ 4, 7]] + [buf[16,19]]
    // [gg[0, 7]]           [buf[ 8,11] [12,15]] [g[ 4, 7]]   [buf[20,23]]

    gf_polymul_64x64_avx2_uvqr_fg(ff,gg,buf,f+4,g+4);
    memcpy((short *)fx+(64-t),ff,256);
    memcpy((short *)gx+(64-t),gg,256);
    memset(uvqr,0,1536);
    memcpy(uvqr,buf,128);
    memcpy(uvqr+8,buf+4,128);
    memcpy(uvqr+16,buf+8,128);
    memcpy(uvqr+24,buf+12,128);
    uvqr[32] = barrett_fake(_mm256_add_epi16(fx[0],buf[16]));
    uvqr[33] = barrett_fake(_mm256_add_epi16(fx[1],buf[17]));
    uvqr[34] = barrett_fake(_mm256_add_epi16(fx[2],buf[18]));
    uvqr[35] = barrett_fake(_mm256_add_epi16(fx[3],buf[19]));
    uvqr[36] = fx[4];
    uvqr[37] = fx[5];
    uvqr[38] = fx[6];
    uvqr[39] = fx[7];
    uvqr[40] = barrett_fake(_mm256_add_epi16(gx[0],buf[20]));
    uvqr[41] = barrett_fake(_mm256_add_epi16(gx[1],buf[21]));
    uvqr[42] = barrett_fake(_mm256_add_epi16(gx[2],buf[22]));
    uvqr[43] = barrett_fake(_mm256_add_epi16(gx[3],buf[23]));
    uvqr[44] = gx[4];
    uvqr[45] = gx[5];
    uvqr[46] = gx[6];
    uvqr[47] = gx[7];
  } else {
    delta = jump64divsteps(delta, f, g, buf);

    // [ff[0, 7]] = [buf[ 0, 3] [ 4, 7]] [f[ 4, 7]] + [buf[16,19]]
    // [gg[0, 7]]   [buf[ 8,11] [12,15]] [g[ 4, 7]]   [buf[20,23]]

    gf_polymul_64x64_avx2_uvqr_fg(ff,gg,buf,f+4,g+4);
    ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[16]));
    ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[17]));
    ff[2] = barrett_fake(_mm256_add_epi16(ff[2],buf[18]));
    ff[3] = barrett_fake(_mm256_add_epi16(ff[3],buf[19]));
    gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[20]));
    gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[21]));
    gg[2] = barrett_fake(_mm256_add_epi16(gg[2],buf[22]));
    gg[3] = barrett_fake(_mm256_add_epi16(gg[3],buf[23]));

    delta = jump64xdivsteps(delta, ff, gg, buf+24, t-64);

    // [uvqr[32,39]]=x^(128-t)[buf[24,27] [28,31]] [ff[ 4, 7]] + [buf[40,43]]
    // [uvqr[40,47]]          [buf[32,35] [36,39]] [gg[ 4, 7]]   [buf[44,47]]

    gf_polymul_64x64_avx2_uvqr_fg_x(uvqr+32,uvqr+40,buf+24,ff+4,gg+4);
    memcpy((short *)fx+(128-t),uvqr+32,256);
    memcpy((short *)gx+(128-t),uvqr+40,256);
    uvqr[32] = barrett_fake(_mm256_add_epi16(fx[0],buf[40]));
    uvqr[33] = barrett_fake(_mm256_add_epi16(fx[1],buf[41]));
    uvqr[34] = barrett_fake(_mm256_add_epi16(fx[2],buf[42]));
    uvqr[35] = barrett_fake(_mm256_add_epi16(fx[3],buf[43]));
    uvqr[36] = fx[4];
    uvqr[37] = fx[5];
    uvqr[38] = fx[6];
    uvqr[39] = fx[7];
    uvqr[40] = barrett_fake(_mm256_add_epi16(gx[0],buf[44]));
    uvqr[41] = barrett_fake(_mm256_add_epi16(gx[1],buf[45]));
    uvqr[42] = barrett_fake(_mm256_add_epi16(gx[2],buf[46]));
    uvqr[43] = barrett_fake(_mm256_add_epi16(gx[3],buf[47]));
    uvqr[44] = gx[4];
    uvqr[45] = gx[5];
    uvqr[46] = gx[6];
    uvqr[47] = gx[7];

    // [uvqr[ 0, 7]  [ 8,15]] = [buf[24,27] [28,31]] [buf[ 0, 3] [ 4, 7]]
    // [uvqr[16,23]  [24,31]] = [buf[32,35] [36,39]] [buf[ 8,11] [12,15]]

    gf_polymul_64x64_avx2_uvqr_vr_x(uvqr,uvqr+16,buf+24,buf,buf+8);
    gf_polymul_64x64_avx2_uvqr_vr_x(uvqr+8,uvqr+24,buf+24,buf+4,buf+12);
  }
  return(delta);
}
int jump256divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr);
int jump256xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int x);
extern int jump128divsteps(int delta, __m256i *f, __m256i *g, __m256i *buf);
extern int jump128xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *buf, int x);

int jump256divsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr){

  __m256i buf[96], ff[16], gg[16];
  __m256i mask, mask1, temp;

  delta = jump128divsteps(delta, f, g, buf);

  // [ff[0,15]] = [buf[ 0, 7] [ 8,15]] [f[ 8,15]] + [buf[32,39]]
  // [gg[0,15]]   [buf[16,23] [24,31]] [g[ 8,15]]   [buf[40,47]]

  gf_polymul_128x128_avx2_uvqr_fg(ff,gg,buf,f+8,g+8);
  ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[32]));
  ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[33]));
  ff[2] = barrett_fake(_mm256_add_epi16(ff[2],buf[34]));
  ff[3] = barrett_fake(_mm256_add_epi16(ff[3],buf[35]));
  ff[4] = barrett_fake(_mm256_add_epi16(ff[4],buf[36]));
  ff[5] = barrett_fake(_mm256_add_epi16(ff[5],buf[37]));
  ff[6] = barrett_fake(_mm256_add_epi16(ff[6],buf[38]));
  ff[7] = barrett_fake(_mm256_add_epi16(ff[7],buf[39]));
  gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[40]));
  gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[41]));
  gg[2] = barrett_fake(_mm256_add_epi16(gg[2],buf[42]));
  gg[3] = barrett_fake(_mm256_add_epi16(gg[3],buf[43]));
  gg[4] = barrett_fake(_mm256_add_epi16(gg[4],buf[44]));
  gg[5] = barrett_fake(_mm256_add_epi16(gg[5],buf[45]));
  gg[6] = barrett_fake(_mm256_add_epi16(gg[6],buf[46]));
  gg[7] = barrett_fake(_mm256_add_epi16(gg[7],buf[47]));

  delta = jump128divsteps(delta, ff, gg, buf+48);

  // [uvqr[64,79]] = [buf[48,55] [56,63]] [ff[ 8,15]] + [buf[80,87]]
  // [uvqr[80,95]]   [buf[64,71] [72,79]] [gg[ 8,15]]   [buf[88,95]]
  gf_polymul_128x128_avx2_uvqr_fg(uvqr+64,uvqr+80,buf+48,ff+8,gg+8);

  uvqr[64] = barrett_fake(_mm256_add_epi16(uvqr[64],buf[80]));
  uvqr[65] = barrett_fake(_mm256_add_epi16(uvqr[65],buf[81]));
  uvqr[66] = barrett_fake(_mm256_add_epi16(uvqr[66],buf[82]));
  uvqr[67] = barrett_fake(_mm256_add_epi16(uvqr[67],buf[83]));
  uvqr[68] = barrett_fake(_mm256_add_epi16(uvqr[68],buf[84]));
  uvqr[69] = barrett_fake(_mm256_add_epi16(uvqr[69],buf[85]));
  uvqr[70] = barrett_fake(_mm256_add_epi16(uvqr[70],buf[86]));
  uvqr[71] = barrett_fake(_mm256_add_epi16(uvqr[71],buf[87]));
  uvqr[80] = barrett_fake(_mm256_add_epi16(uvqr[80],buf[88]));
  uvqr[81] = barrett_fake(_mm256_add_epi16(uvqr[81],buf[89]));
  uvqr[82] = barrett_fake(_mm256_add_epi16(uvqr[82],buf[90]));
  uvqr[83] = barrett_fake(_mm256_add_epi16(uvqr[83],buf[91]));
  uvqr[84] = barrett_fake(_mm256_add_epi16(uvqr[84],buf[92]));
  uvqr[85] = barrett_fake(_mm256_add_epi16(uvqr[85],buf[93]));
  uvqr[86] = barrett_fake(_mm256_add_epi16(uvqr[86],buf[94]));
  uvqr[87] = barrett_fake(_mm256_add_epi16(uvqr[87],buf[95]));

  // [uvqr[ 0,15]  [16,31]] = [buf[48,55] [56,63]] [buf[ 0, 7] [ 8,15]]
  // [uvqr[32,47]  [48,63]] = [buf[64,71] [72,79]] [buf[16,23] [24,31]]

  gf_polymul_128x128_avx2_uvqr_vr(uvqr,uvqr+32,buf+48,buf,buf+16);
  gf_polymul_128x128_avx2_uvqr_vr(uvqr+16,uvqr+48,buf+48,buf+8,buf+24);

  return(delta);
}


int jump256xdivsteps(int delta, __m256i *f, __m256i *g, __m256i *uvqr, int t){

  __m256i buf[96], ff[16], gg[16];
  __m256i fx[32], gx[32];
  __m256i mask,mask1,temp;
  int i;

  memset(fx,0,sizeof(fx)); memset(gx,0,sizeof(gx));

  if (t <= 128){
    delta = jump128xdivsteps(delta, f, g, buf, t);

    // [ff[0,15]]=x^(128-t) [buf[ 0, 7] [ 8,15]] [f[ 8,15]] + [buf[32,39]]
    // [gg[0,15]]           [buf[16,23] [24,31]] [g[ 8,15]]   [buf[40,47]]

    gf_polymul_128x128_avx2_uvqr_fg(ff,gg,buf,f+8,g+8);
    memcpy((short *)fx+(128-t),ff,512);
    memcpy((short *)gx+(128-t),gg,512);
    memset(uvqr,0,3072);
    memcpy(uvqr,buf,256);
    memcpy(uvqr+16,buf+8,256);
    memcpy(uvqr+32,buf+16,256);
    memcpy(uvqr+48,buf+24,256);
    uvqr[64] = barrett_fake(_mm256_add_epi16(fx[0],buf[32]));
    uvqr[65] = barrett_fake(_mm256_add_epi16(fx[1],buf[33]));
    uvqr[66] = barrett_fake(_mm256_add_epi16(fx[2],buf[34]));
    uvqr[67] = barrett_fake(_mm256_add_epi16(fx[3],buf[35]));
    uvqr[68] = barrett_fake(_mm256_add_epi16(fx[4],buf[36]));
    uvqr[69] = barrett_fake(_mm256_add_epi16(fx[5],buf[37]));
    uvqr[70] = barrett_fake(_mm256_add_epi16(fx[6],buf[38]));
    uvqr[71] = barrett_fake(_mm256_add_epi16(fx[7],buf[39]));
    uvqr[72] = fx[8];
    uvqr[73] = fx[9];
    uvqr[74] = fx[10];
    uvqr[75] = fx[11];
    uvqr[76] = fx[12];
    uvqr[77] = fx[13];
    uvqr[78] = fx[14];
    uvqr[79] = fx[15];
    uvqr[80] = barrett_fake(_mm256_add_epi16(gx[0],buf[40]));
    uvqr[81] = barrett_fake(_mm256_add_epi16(gx[1],buf[41]));
    uvqr[82] = barrett_fake(_mm256_add_epi16(gx[2],buf[42]));
    uvqr[83] = barrett_fake(_mm256_add_epi16(gx[3],buf[43]));
    uvqr[84] = barrett_fake(_mm256_add_epi16(gx[4],buf[44]));
    uvqr[85] = barrett_fake(_mm256_add_epi16(gx[5],buf[45]));
    uvqr[86] = barrett_fake(_mm256_add_epi16(gx[6],buf[46]));
    uvqr[87] = barrett_fake(_mm256_add_epi16(gx[7],buf[47]));
    uvqr[88] = gx[8];
    uvqr[89] = gx[9];
    uvqr[90] = gx[10];
    uvqr[91] = gx[11];
    uvqr[92] = gx[12];
    uvqr[93] = gx[13];
    uvqr[94] = gx[14];
    uvqr[95] = gx[15];
  } else {
    delta = jump128divsteps(delta, f, g, buf);

    // [ff[0,15]] = [buf[ 0, 7] [ 8,15]] [f[ 8,15]] + [buf[32,39]]
    // [gg[0,15]]   [buf[16,23] [24,31]] [g[ 8,15]]   [buf[40,47]]

    gf_polymul_128x128_avx2_uvqr_fg(ff,gg,buf,f+8,g+8);
    ff[0] = barrett_fake(_mm256_add_epi16(ff[0],buf[32]));
    ff[1] = barrett_fake(_mm256_add_epi16(ff[1],buf[33]));
    ff[2] = barrett_fake(_mm256_add_epi16(ff[2],buf[34]));
    ff[3] = barrett_fake(_mm256_add_epi16(ff[3],buf[35]));
    ff[4] = barrett_fake(_mm256_add_epi16(ff[4],buf[36]));
    ff[5] = barrett_fake(_mm256_add_epi16(ff[5],buf[37]));
    ff[6] = barrett_fake(_mm256_add_epi16(ff[6],buf[38]));
    ff[7] = barrett_fake(_mm256_add_epi16(ff[7],buf[39]));
    gg[0] = barrett_fake(_mm256_add_epi16(gg[0],buf[40]));
    gg[1] = barrett_fake(_mm256_add_epi16(gg[1],buf[41]));
    gg[2] = barrett_fake(_mm256_add_epi16(gg[2],buf[42]));
    gg[3] = barrett_fake(_mm256_add_epi16(gg[3],buf[43]));
    gg[4] = barrett_fake(_mm256_add_epi16(gg[4],buf[44]));
    gg[5] = barrett_fake(_mm256_add_epi16(gg[5],buf[45]));
    gg[6] = barrett_fake(_mm256_add_epi16(gg[6],buf[46]));
    gg[7] = barrett_fake(_mm256_add_epi16(gg[7],buf[47]));

    delta = jump128xdivsteps(delta, ff, gg, buf+48, t-128);

    // [uvqr[64,79]]=x^(256-t)[buf[48,55] [56,63]] [ff[ 8,15]] + [buf[80,87]]
    // [uvqr[80,95]]          [buf[64,71] [72,79]] [gg[ 8,15]]   [buf[88,95]]

    gf_polymul_128x128_avx2_uvqr_fg_x(uvqr+64,uvqr+80,buf+48,ff+8,gg+8);
    memcpy((short *)fx+(256-t),uvqr+64,512);
    memcpy((short *)gx+(256-t),uvqr+80,512);
    uvqr[64] = barrett_fake(_mm256_add_epi16(fx[0],buf[80]));
    uvqr[65] = barrett_fake(_mm256_add_epi16(fx[1],buf[81]));
    uvqr[66] = barrett_fake(_mm256_add_epi16(fx[2],buf[82]));
    uvqr[67] = barrett_fake(_mm256_add_epi16(fx[3],buf[83]));
    uvqr[68] = barrett_fake(_mm256_add_epi16(fx[4],buf[84]));
    uvqr[69] = barrett_fake(_mm256_add_epi16(fx[5],buf[85]));
    uvqr[70] = barrett_fake(_mm256_add_epi16(fx[6],buf[86]));
    uvqr[71] = barrett_fake(_mm256_add_epi16(fx[7],buf[87]));
    uvqr[72] = fx[8];
    uvqr[73] = fx[9];
    uvqr[74] = fx[10];
    uvqr[75] = fx[11];
    uvqr[76] = fx[12];
    uvqr[77] = fx[13];
    uvqr[78] = fx[14];
    uvqr[79] = fx[15];
    uvqr[80] = barrett_fake(_mm256_add_epi16(gx[0],buf[88]));
    uvqr[81] = barrett_fake(_mm256_add_epi16(gx[1],buf[89]));
    uvqr[82] = barrett_fake(_mm256_add_epi16(gx[2],buf[90]));
    uvqr[83] = barrett_fake(_mm256_add_epi16(gx[3],buf[91]));
    uvqr[84] = barrett_fake(_mm256_add_epi16(gx[4],buf[92]));
    uvqr[85] = barrett_fake(_mm256_add_epi16(gx[5],buf[93]));
    uvqr[86] = barrett_fake(_mm256_add_epi16(gx[6],buf[94]));
    uvqr[87] = barrett_fake(_mm256_add_epi16(gx[7],buf[95]));
    uvqr[88] = gx[8];
    uvqr[89] = gx[9];
    uvqr[90] = gx[10];
    uvqr[91] = gx[11];
    uvqr[92] = gx[12];
    uvqr[93] = gx[13];
    uvqr[94] = gx[14];
    uvqr[95] = gx[15];

    // [uvqr[ 0,15]  [16,31]] = [buf[48,55] [56,63]] [buf[ 0, 7] [ 8,15]]
    // [uvqr[32,47]  [48,63]] = [buf[64,71] [72,79]] [buf[16,23] [24,31]]

    gf_polymul_128x128_avx2_uvqr_vr_x(uvqr,uvqr+32,buf+48,buf,buf+16);
    gf_polymul_128x128_avx2_uvqr_vr_x(uvqr+16,uvqr+48,buf+48,buf+8,buf+24);
  }
  return(delta);
}
