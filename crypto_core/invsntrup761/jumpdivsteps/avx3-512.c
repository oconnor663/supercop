/* auto-generated; do not edit */

#include <stdint.h>
#include <immintrin.h>

#include "ntt.h"

typedef int8_t int8;
typedef int16_t int16;

#define zeta(n,i) (i[(__m256i *) zeta_##n])
#define zeta_x4(n,i) (i[(__m256i *) zeta_x4_##n])
#define zeta_qinv(n,i) (i[(__m256i *) qinvzeta_##n])
#define zeta_x4_qinv(n,i) (i[(__m256i *) qinvzeta_x4_##n])
#define zetainv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) zeta_##n+n/2+1-16*(i+1)))
#define zetainv_x4(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) zeta_x4_##n+2*n+4-16*(i+1)))
#define zetainv_qinv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) qinvzeta_##n+n/2+1-16*(i+1)))
#define zetainv_x4_qinv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) qinvzeta_x4_##n+2*n+4-16*(i+1)))

static const __attribute((aligned(32))) int16 qdata_7681[] = {

#define q_x16 (qdata[0])
  7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,

#define qrecip_x16 (qdata[1])
  17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,17473,

#define qshift_x16 (qdata[2])
  16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,

#define p2inv_512 (qdata[3])
  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

#define p2inv_512_qinv (qdata[4])
  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

#define zeta4_x16 (qdata[5])
  -3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,

#define zeta4_x16_qinv (qdata[6])
  -28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,

#define zeta8_x16 (qdata[7])
  -3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,

#define zeta8_x16_qinv (qdata[8])
  -16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,

#define zetainv8_x16 (qdata[9])
  -3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,

#define zetainv8_x16_qinv (qdata[10])
  -10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,

#define zeta_x4_16 (qdata+11)
  -3593,-3593,-3593,-3593,-2194,-2194,-2194,-2194,-3625,-3625,-3625,-3625,1100,1100,1100,1100,
  -3777,-3777,-3777,-3777,-2456,-2456,-2456,-2456,3182,3182,3182,3182,3696,3696,3696,3696,
  3593,3593,3593,3593,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_x4_16 (qdata+14)
  -9,-9,-9,-9,4974,4974,4974,4974,-16425,-16425,-16425,-16425,7244,7244,7244,7244,
  -28865,-28865,-28865,-28865,-14744,-14744,-14744,-14744,10350,10350,10350,10350,-4496,-4496,-4496,-4496,
  9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_x4_32 (qdata+17)
  -3593,-3593,-3593,-3593,1414,1414,1414,1414,-2194,-2194,-2194,-2194,-2495,-2495,-2495,-2495,
  -3625,-3625,-3625,-3625,2876,2876,2876,2876,1100,1100,1100,1100,-2250,-2250,-2250,-2250,
  -3777,-3777,-3777,-3777,-1701,-1701,-1701,-1701,-2456,-2456,-2456,-2456,834,834,834,834,
  3182,3182,3182,3182,-2319,-2319,-2319,-2319,3696,3696,3696,3696,121,121,121,121,
  3593,3593,3593,3593,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_x4_32 (qdata+22)
  -9,-9,-9,-9,20870,20870,20870,20870,4974,4974,4974,4974,22593,22593,22593,22593,
  -16425,-16425,-16425,-16425,828,828,828,828,7244,7244,7244,7244,-23754,-23754,-23754,-23754,
  -28865,-28865,-28865,-28865,20315,20315,20315,20315,-14744,-14744,-14744,-14744,18242,18242,18242,18242,
  10350,10350,10350,10350,-18191,-18191,-18191,-18191,-4496,-4496,-4496,-4496,-11655,-11655,-11655,-11655,
  9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_64 (qdata+27)
  -3593,-617,1414,3706,-2194,-1296,-2495,-2237,-3625,2830,2876,-1599,1100,1525,-2250,2816,
  -3777,1921,-1701,2006,-2456,1483,834,-1986,3182,3364,-2319,-1993,3696,-2557,121,2088,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_64 (qdata+30)
  -9,19351,20870,-15750,4974,-9488,22593,7491,-16425,26382,828,23489,7244,20469,-23754,2816,
  -28865,-5759,20315,-3114,-14744,15307,18242,-19394,10350,-10972,-18191,-31177,-4496,-25597,-11655,22568,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_128 (qdata+33)
  -3593,-2804,-617,-396,1414,-549,3706,810,-2194,-1321,-1296,438,-2495,-2535,-2237,-3689,
  -3625,2043,2830,-1881,2876,3153,-1599,7,1100,-514,1525,-1760,-2250,-2440,2816,3600,
  -3777,103,1921,-3174,-1701,1535,2006,-1887,-2456,1399,1483,-679,834,3772,-1986,1738,
  3182,-1431,3364,-3555,-2319,-2310,-1993,638,3696,-2956,-2557,-1305,121,2555,2088,-3266,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_128 (qdata+38)
  -9,-29428,19351,26228,20870,21467,-15750,5930,4974,-14121,-9488,-21066,22593,2073,7491,16279,
  -16425,-25093,26382,26279,828,-29103,23489,11783,7244,14846,20469,14624,-23754,-6536,2816,11792,
  -28865,-4505,-5759,-6246,20315,9215,-3114,6817,-14744,4983,15307,-28839,18242,1724,-19394,23242,
  10350,-21399,-10972,-29667,-18191,-21766,-31177,15998,-4496,23668,-25597,-5913,-11655,-24581,22568,-20674,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_256 (qdata+43)
  -3593,2665,-2804,-2572,-617,727,-396,3417,1414,2579,-549,373,3706,3750,810,-1054,
  -2194,-2133,-1321,1681,-1296,-1386,438,-2732,-2495,1919,-2535,-2391,-2237,2835,-3689,2,
  -3625,-783,2043,3145,2830,1533,-1881,2789,2876,2649,3153,3692,-1599,-1390,7,-1166,
  1100,3310,-514,2224,1525,-2743,-1760,2385,-2250,-486,-2440,-1756,2816,-3816,3600,-3831,
  -3777,-1799,103,1497,1921,1521,-3174,-194,-1701,-859,1535,2175,2006,-2762,-1887,-1698,
  -2456,-3480,1399,2883,1483,-3428,-679,-2113,834,1532,3772,-660,-1986,-2764,1738,-915,
  3182,1056,-1431,1350,3364,1464,-3555,2919,-2319,-2160,-2310,730,-1993,-1598,638,3456,
  3696,-1168,-2956,-3588,-2557,-921,-1305,3405,121,-404,2555,-3135,2088,2233,-3266,-2426,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_256 (qdata+52)
  -9,-17303,-29428,24052,19351,-12073,26228,-24743,20870,-12269,21467,19317,-15750,-25946,5930,32738,
  4974,-4693,-14121,2193,-9488,26262,-21066,7508,22593,9599,2073,10409,7491,-12013,16279,-15358,
  -16425,-16655,-25093,32329,26382,24573,26279,13541,828,-25511,-29103,26220,23489,-8558,11783,-24718,
  7244,10478,14846,26800,20469,26441,14624,-29871,-23754,-3558,-6536,-16092,2816,8472,11792,-7415,
  -28865,-13575,-4505,-26663,-5759,-14351,-6246,-17602,20315,-22875,9215,9855,-3114,-24266,6817,-2722,
  -14744,-15768,4983,12611,15307,-21860,-28839,-27201,18242,32252,1724,21868,-19394,-8908,23242,13933,
  10350,17440,-21399,-11962,-10972,30136,-29667,-1689,-18191,6032,-21766,30426,-31177,15810,15998,3456,
  -4496,-9360,23668,27132,-25597,-5529,-5913,1869,-11655,22124,-24581,21953,22568,23225,-20674,17030,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_512 (qdata+61)
  -3593,2005,2665,2891,-2804,2345,-2572,1121,-617,-188,727,2786,-396,-3208,3417,-17,
  1414,-3752,2579,2815,-549,1837,373,151,3706,-1012,3750,-1509,810,-3214,-1054,3177,
  -2194,-1403,-2133,-3314,-1321,83,1681,-658,-1296,2070,-1386,-3547,438,3781,-2732,2230,
  -2495,-1669,1919,2589,-2535,-3312,-2391,-3542,-2237,-1441,2835,-3568,-3689,-402,2,-1070,
  -3625,3763,-783,-3550,2043,-2303,3145,-436,2830,-893,1533,1712,-1881,124,2789,-2001,
  2876,-2460,2649,3770,3153,2965,3692,-1203,-1599,2874,-1390,-1407,7,-3745,-1166,1649,
  1100,2937,3310,3461,-514,-1526,2224,715,1525,-1689,-2743,434,-1760,-3163,2385,-929,
  -2250,-2167,-486,-1144,-2440,-370,-1756,2378,2816,-1084,-3816,-1586,3600,1931,-3831,-1242,
  -3777,592,-1799,2340,103,-1338,1497,-2071,1921,1519,1521,451,-3174,589,-194,-3744,
  -1701,3677,-859,-1295,1535,642,2175,-3794,2006,2130,-2762,2918,-1887,3334,-1698,2072,
  -2456,509,-3480,2998,1399,-3408,2883,1476,1483,-2262,-3428,-1779,-679,2258,-2113,1348,
  834,-692,1532,2247,3772,2083,-660,-226,-1986,2532,-2764,-3693,1738,-429,-915,-2059,
  3182,2812,1056,3434,-1431,-2515,1350,-236,3364,-2386,1464,222,-3555,-2963,2919,-2422,
  -2319,-3657,-2160,3450,-2310,-791,730,1181,-1993,-1404,-1598,2339,638,-3366,3456,2161,
  3696,-3343,-1168,2719,-2956,-826,-3588,-670,-2557,777,-921,1151,-1305,-796,3405,-1278,
  121,-3287,-404,1072,2555,293,-3135,2767,2088,-3335,2233,3581,-3266,3723,-2426,-179,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_512 (qdata+78)
  -9,4565,-17303,16715,-29428,15145,24052,-22943,19351,1860,-12073,-28958,26228,-7304,-24743,-529,
  20870,-24232,-12269,10495,21467,-16083,19317,20119,-15750,-27636,-25946,-12261,5930,-26766,32738,-16791,
  4974,25733,-4693,20238,-14121,18003,2193,6510,-9488,29718,26262,-25563,-21066,-1851,7508,-19274,
  22593,-28805,9599,-23523,2073,4880,10409,1578,7491,-10145,-12013,4624,16279,6766,-15358,24530,
  -16425,5299,-16655,-2526,-25093,-9983,32329,5708,26382,-23933,24573,26288,26279,30844,13541,30255,
  828,15972,-25511,17082,-29103,-27243,26220,-2739,23489,16186,-8558,-9087,11783,-12449,-24718,-14223,
  7244,-8839,10478,30597,14846,-12790,26800,14539,20469,-6297,26441,9650,14624,-25179,-29871,-9633,
  -23754,-5751,-3558,2952,-6536,23182,-16092,23882,2816,964,8472,-10802,11792,-17013,-7415,-30938,
  -28865,-23984,-13575,-11996,-4505,-14650,-26663,-22039,-5759,1007,-14351,10179,-6246,-947,-17602,-20128,
  20315,10333,-22875,-17167,9215,-14718,9855,-29394,-3114,27730,-24266,5990,6817,22790,-2722,14360,
  -14744,23549,-15768,-18506,4983,21168,12611,3524,15307,2858,-21860,29453,-28839,27858,-27201,3396,
  18242,5452,32252,-18745,1724,-4573,21868,31518,-19394,20964,-8908,-18541,23242,17491,13933,16885,
  10350,-32004,17440,-24214,-21399,-20435,-11962,-22764,-10972,-27986,30136,-802,-29667,11885,-1689,-13686,
  -18191,32695,6032,-16006,-21766,-20759,30426,-24931,-31177,-32124,15810,-4317,15998,26330,3456,-13711,
  -4496,-19215,-9360,26783,23668,-14138,27132,-32414,-25597,-2807,-5529,8831,-5913,17636,1869,-16638,
  -11655,9513,22124,25648,-24581,-21723,21953,-14129,22568,-15111,23225,26621,-20674,-15221,17030,-1715,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

};



static const __attribute((aligned(32))) int16 qdata_10753[] = {

  10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,

  24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,24963,

  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,

  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

  223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,

  27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,

  4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,

  -1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,

  3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,

  -408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,

  1018,1018,1018,1018,2413,2413,2413,2413,4188,4188,4188,4188,357,357,357,357,
  223,223,223,223,-3686,-3686,-3686,-3686,-3688,-3688,-3688,-3688,-376,-376,-376,-376,
  -1018,-1018,-1018,-1018,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-6,-6,-6,10093,10093,10093,10093,-1956,-1956,-1956,-1956,28517,28517,28517,28517,
  27359,27359,27359,27359,-21094,-21094,-21094,-21094,408,408,408,408,-20856,-20856,-20856,-20856,
  6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,1018,1018,1018,-2695,-2695,-2695,-2695,2413,2413,2413,2413,425,425,425,425,
  4188,4188,4188,4188,-4855,-4855,-4855,-4855,357,357,357,357,-3364,-3364,-3364,-3364,
  223,223,223,223,730,730,730,730,-3686,-3686,-3686,-3686,-4544,-4544,-4544,-4544,
  -3688,-3688,-3688,-3688,-2236,-2236,-2236,-2236,-376,-376,-376,-376,3784,3784,3784,3784,
  -1018,-1018,-1018,-1018,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-6,-6,-6,7033,7033,7033,7033,10093,10093,10093,10093,18345,18345,18345,18345,
  -1956,-1956,-1956,-1956,29449,29449,29449,29449,28517,28517,28517,28517,-9508,-9508,-9508,-9508,
  27359,27359,27359,27359,16090,16090,16090,16090,-21094,-21094,-21094,-21094,28224,28224,28224,28224,
  408,408,408,408,-12476,-12476,-12476,-12476,-20856,-20856,-20856,-20856,16072,16072,16072,16072,
  6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-1520,-2695,1341,2413,918,425,5175,4188,-4035,-4855,341,357,4347,-3364,5213,
  223,-4875,730,1931,-3686,-2503,-4544,-4095,-3688,5063,-2236,-3823,-376,3012,3784,-2629,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,23056,7033,829,10093,26518,18345,3639,-1956,-4547,29449,3925,28517,-7429,-9508,-11683,
  27359,-17675,16090,14731,-21094,-25543,28224,-14847,408,28103,-12476,10001,-20856,-7228,16072,18363,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-2935,-1520,-4744,-2695,-205,1341,1299,2413,4,918,-4379,425,-4616,5175,-544,
  4188,4129,-4035,4102,-4855,-1287,341,-2388,357,1284,4347,2984,-3364,2178,5213,-2576,
  223,2790,-4875,4876,730,-4513,1931,-3085,-3686,3550,-2503,847,-4544,193,-4095,1085,
  -3688,3091,5063,-4742,-2236,2982,-3823,-1009,-376,-268,3012,3062,3784,-2565,-2629,4189,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,31369,23056,15736,7033,-24269,829,-6381,10093,22532,26518,23781,18345,15864,3639,15840,
  -1956,-23007,-4547,5126,29449,8441,3925,-16724,28517,23812,-7429,31656,-9508,-19326,-11683,-27152,
  27359,20198,-17675,6924,16090,22623,14731,5619,-21094,-24098,-25543,3407,28224,22209,-14847,573,
  408,-4589,28103,-5766,-12476,-12378,10001,-31217,-20856,-2316,-7228,-20490,16072,-14341,18363,-12707,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-4734,-2935,-400,-1520,4977,-4744,-2973,-2695,512,-205,-779,1341,-1356,1299,635,
  2413,567,4,-4286,918,-5114,-4379,-1586,425,1615,-4616,-336,5175,-1841,-544,2234,
  4188,-3441,4129,636,-4035,-4580,4102,2684,-4855,3057,-1287,-2740,341,-5156,-2388,-472,
  357,-794,1284,578,4347,3615,2984,-3715,-3364,2271,2178,-326,5213,454,-2576,-3337,
  223,2998,2790,-151,-4875,2981,4876,1324,730,2774,-4513,2206,1931,886,-3085,-970,
  -3686,3198,3550,2737,-2503,-909,847,1068,-4544,-2213,193,2884,-4095,-4808,1085,4123,
  -3688,5341,3091,5294,5063,-116,-4742,-5116,-2236,-2045,2982,-1572,-3823,4828,-1009,467,
  -376,5023,-268,-3169,3012,-1458,3062,-1268,3784,-675,-2565,1006,-2629,5064,4189,864,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-26238,31369,-24976,23056,-30351,15736,-18845,7033,512,-24269,-13579,829,29364,-6381,-11141,
  10093,-969,22532,6978,26518,-4090,23781,11726,18345,4175,15864,7856,3639,719,15840,-31558,
  -1956,31887,-23007,-21892,-4547,22044,5126,-19844,29449,-32271,8441,32076,3925,-11300,-16724,28200,
  28517,16614,23812,11842,-7429,-2017,31656,28541,-9508,29407,-19326,31418,-11683,-31290,-27152,27895,
  27359,12214,20198,-14999,-17675,-1627,6924,-13012,16090,-4394,22623,7326,14731,-22666,5619,8246,
  -21094,24702,-24098,177,-25543,7795,3407,-13268,28224,2395,22209,-7356,-14847,-17096,573,-24037,
  408,-11555,-4589,-30546,28103,1932,-5766,17412,-12476,31235,-12378,-7716,10001,-1316,-31217,25555,
  -20856,-609,-2316,-8801,-7228,11854,-20490,780,16072,-17571,-14341,-2066,18363,17352,-12707,17248,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,3453,-4734,4519,-2935,2118,-400,-554,-1520,2196,4977,1893,-4744,-1409,-2973,-1053,
  -2695,4601,512,279,-205,-3241,-779,4889,1341,3524,-1356,-1663,1299,2283,635,73,
  2413,2428,567,624,4,-1930,-4286,3419,918,-2062,-5114,5068,-4379,-97,-1586,1782,
  425,4621,1615,355,-4616,1349,-336,825,5175,3135,-1841,1160,-544,4408,2234,-2605,
  4188,854,-3441,-1056,4129,2439,636,4967,-4035,-4782,-4580,-5268,4102,-663,2684,-4670,
  -4855,3760,3057,3535,-1287,2680,-2740,-569,341,2139,-5156,3827,-2388,1639,-472,1927,
  357,5172,-794,-4003,1284,4144,578,693,4347,4784,3615,3125,2984,1122,-3715,2113,
  -3364,-573,2271,-4328,2178,2909,-326,-4000,5213,-4447,454,-3995,-2576,-4428,-3337,2529,
  223,5309,2998,5120,2790,-2050,-151,2963,-4875,2657,2981,-2807,4876,2237,1324,-4403,
  730,2624,2774,-5083,-4513,40,2206,152,1931,-1573,886,2625,-3085,-778,-970,-5107,
  -3686,4250,3198,-5356,3550,-3148,2737,-3360,-2503,-2015,-909,3096,847,5313,1068,834,
  -4544,-1132,-2213,-2151,193,-1722,2884,-4393,-4095,2662,-4808,-2788,1085,-1992,4123,5334,
  -3688,5215,5341,-1689,3091,-2117,5294,4859,5063,3410,-116,2205,-4742,-2374,-5116,-4720,
  -2236,3570,-2045,2813,2982,2087,-1572,-4973,-3823,458,4828,3891,-1009,-2419,467,-4891,
  -376,-1381,5023,1204,-268,274,-3169,-3260,3012,-1635,-1458,4540,3062,-4254,-1268,-1111,
  3784,2230,-675,-2279,-2565,-4359,1006,-1510,-2629,5015,5064,-2449,4189,-5005,864,2487,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-29827,-26238,-21593,31369,-29626,-24976,-7722,23056,-16236,-30351,30053,15736,9343,-18845,-16925,
  7033,14329,512,15127,-24269,-21161,-13579,-1767,829,-6716,29364,-12415,-6381,31467,-11141,1609,
  10093,-20100,-969,-23952,22532,-25482,6978,8027,26518,17394,-4090,-25652,23781,-5729,11726,-21770,
  18345,-4083,4175,-15517,15864,-19643,7856,-22215,3639,-18881,719,-19320,15840,-7880,-31558,22483,
  -1956,-6314,31887,15328,-23007,-7289,-21892,11623,-4547,31058,22044,13164,5126,-15511,-19844,6594,
  29449,11952,-32271,6095,8441,23160,32076,22471,3925,6747,-11300,12531,-16724,8295,28200,-7801,
  28517,-29644,16614,-20899,23812,12336,11842,20661,-7429,12976,-2017,23093,31656,-3998,28541,24129,
  -9508,-61,29407,-232,-19326,-13987,31418,12384,-11683,-31583,-31290,24165,-27152,26292,27895,8161,
  27359,4797,12214,5120,20198,19454,-14999,-4717,-17675,8289,-1627,31497,6924,1725,-13012,19661,
  16090,-30144,-4394,-9691,22623,28712,7326,4248,14731,3035,-22666,24641,5619,-24330,8246,-13811,
  -21094,-13158,24702,-23788,-24098,27572,177,13024,-25543,-29151,7795,7192,3407,27329,-13268,12098,
  28224,-19564,2395,-8807,22209,32070,-7356,-22313,-14847,20070,-17096,23836,573,-14280,-24037,-1834,
  408,32351,-11555,4967,-4589,18875,-30546,-6917,28103,-26286,1932,18077,-5766,29370,17412,19856,
  -12476,23026,31235,-30467,-12378,-24025,-7716,-12653,10001,-8758,-1316,-20173,-31217,-11123,25555,23269,
  -20856,-29541,-609,31924,-2316,3346,-8801,-13500,-7228,14237,11854,14780,-20490,-9374,780,16809,
  16072,11446,-17571,-8935,-14341,5369,-2066,-18918,18363,19863,17352,-16273,-12707,3699,17248,951,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

};



static const __attribute((aligned(32))) int16 qdata_12289[] = {

  12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,12289,

  21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,21843,

  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,

  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

  128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,

  -4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,-4401,

  7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,7887,

  -1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,-1229,

  27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,27443,

  1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,1081,

  21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,21561,

  4091,4091,4091,4091,-5329,-5329,-5329,-5329,-1229,-1229,-1229,-1229,-6014,-6014,-6014,-6014,
  -4401,-4401,-4401,-4401,4342,4342,4342,4342,-1081,-1081,-1081,-1081,-2530,-2530,-2530,-2530,
  -4091,-4091,-4091,-4091,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,-5,-5,-5,6959,6959,6959,6959,27443,27443,27443,27443,-30590,-30590,-30590,-30590,
  7887,7887,7887,7887,-3850,-3850,-3850,-3850,-21561,-21561,-21561,-21561,22046,22046,22046,22046,
  5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,

  4091,4091,4091,4091,1591,1591,1591,1591,-5329,-5329,-5329,-5329,586,586,586,586,
  -1229,-1229,-1229,-1229,-2812,-2812,-2812,-2812,-6014,-6014,-6014,-6014,-4751,-4751,-4751,-4751,
  -4401,-4401,-4401,-4401,-5890,-5890,-5890,-5890,4342,4342,4342,4342,5825,5825,5825,5825,
  -1081,-1081,-1081,-1081,5266,5266,5266,5266,-2530,-2530,-2530,-2530,-2579,-2579,-2579,-2579,
  -4091,-4091,-4091,-4091,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,-5,-5,-5,-18889,-18889,-18889,-18889,6959,6959,6959,6959,8778,8778,8778,8778,
  27443,27443,27443,27443,13572,13572,13572,13572,-30590,-30590,-30590,-30590,-17039,-17039,-17039,-17039,
  7887,7887,7887,7887,18686,18686,18686,18686,-3850,-3850,-3850,-3850,-6463,-6463,-6463,-6463,
  -21561,-21561,-21561,-21561,-19310,-19310,-19310,-19310,22046,22046,22046,22046,-31251,-31251,-31251,-31251,
  5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,

  4091,1134,1591,-1875,-5329,-5190,586,1364,-1229,1711,-2812,1885,-6014,3743,-4751,-2125,
  -4401,-5882,-5890,-4189,4342,-4615,5825,-1960,-1081,965,5266,1688,-2530,-5847,-2579,-3109,
  -4091,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,25710,-18889,-30547,6959,3002,8778,17748,27443,13999,13572,-26787,-30590,16031,-17039,26547,
  7887,-14074,18686,24483,-3850,15865,-6463,30808,-21561,5061,-19310,-31080,22046,14633,-31251,-7205,
  5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  4091,-79,1134,-100,1591,2829,-1875,2181,-5329,4783,-5190,-4679,586,-5145,1364,4843,
  -1229,997,1711,-1538,-2812,4431,1885,-3569,-6014,1549,3743,3983,-4751,5664,-2125,14,
  -4401,-6049,-5882,432,-5890,-5831,-4189,-5981,4342,4407,-4615,1534,5825,2564,-1960,1690,
  -1081,117,965,1237,5266,-3412,1688,-5719,-2530,-5217,-5847,-4426,-2579,4042,-3109,3872,
  -4091,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,-12367,25710,-16484,-18889,-25843,-30547,6277,6959,17071,3002,15801,8778,-25625,17748,747,
  27443,5093,13999,23038,13572,16719,-26787,8719,-30590,-27123,16031,16271,-17039,5664,26547,24590,
  7887,6239,-14074,432,18686,14649,24483,22691,-3850,-16073,15865,26110,-6463,18948,30808,9882,
  -21561,4213,5061,5333,-19310,-19796,-31080,14761,22046,7071,14633,-12618,-31251,12234,-7205,3872,
  5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  4091,5569,-79,-1228,1134,3316,-100,-3110,1591,1553,2829,730,-1875,-3012,2181,-4676,
  -5329,-4746,4783,4970,-5190,-1652,-4679,3180,586,1020,-5145,5892,1364,-1820,4843,-5453,
  -1229,-126,997,2742,1711,5285,-1538,3782,-2812,-3888,4431,3854,1885,3323,-3569,834,
  -6014,4673,1549,-2211,3743,-2796,3983,4668,-4751,-1517,5664,-3269,-2125,1502,14,5351,
  -4401,-2921,-6049,-2560,-5882,-1053,432,3604,-5890,1156,-5831,1762,-4189,6130,-5981,-2903,
  4342,2315,4407,-1808,-4615,-2203,1534,3467,5825,2967,2564,-1367,-1960,489,1690,3403,
  -1081,2019,117,-48,965,-711,1237,-2083,5266,-900,-3412,2030,1688,883,-5719,-4586,
  -2530,-4949,-5217,1195,-5847,-6109,-4426,2446,-2579,-5244,4042,5274,-3109,2851,3872,-13,
  -4091,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,-6719,-12367,15156,25710,19700,-16484,5082,-18889,-10735,-25843,8922,-30547,-19396,6277,-21060,
  6959,-12938,17071,13162,3002,-18036,15801,-13204,8778,-15364,-25625,22276,17748,14564,747,23219,
  27443,-24702,5093,-5450,13999,9381,23038,-4410,13572,-3888,16719,28430,-26787,-773,8719,-23742,
  -30590,-7615,-27123,-30883,16031,13588,16271,-11716,-17039,27155,5664,-7365,26547,26078,24590,-15129,
  7887,-23401,6239,-2560,-14074,27619,432,19988,18686,17540,14649,-22814,24483,-18446,22691,17577,
  -3850,-1781,-16073,-1808,15865,1893,26110,-629,-6463,-17513,18948,19113,30808,20969,9882,-693,
  -21561,30691,4213,-48,5061,19769,5333,-30755,-19310,-17284,-19796,26606,-31080,29555,14761,-12778,
  22046,-9045,7071,-2901,14633,22563,-12618,27022,-31251,11140,12234,13466,-7205,31523,3872,28659,
  5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  4091,3580,5569,737,-79,-5344,-1228,932,1134,-5424,3316,-1556,-100,5680,-3110,4602,
  1591,-1888,1553,5186,2829,-3388,730,-4597,-1875,-4101,-3012,-5880,2181,1467,-4676,5070,
  -5329,-2080,-4746,-3243,4783,6057,4970,351,-5190,-144,-1652,2895,-4679,-2133,3180,3711,
  586,6040,1020,3509,-5145,-2700,5892,2053,1364,6090,-1820,5064,4843,2649,-5453,-4868,
  -1229,-1469,-126,4699,997,-2558,2742,-3362,1711,3585,5285,-5252,-1538,-6038,3782,-989,
  -2812,-4951,-3888,4552,4431,-3443,3854,-163,1885,3533,3323,2962,-3569,-3736,834,-673,
  -6014,-39,4673,16,1549,-4418,-2211,237,3743,3684,-2796,-3402,3983,2341,4668,300,
  -4751,-2959,-1517,-4773,5664,-4659,-3269,3802,-2125,-2190,1502,5625,14,-3253,5351,5746,
  -4401,1739,-2921,3698,-6049,1949,-2560,-2060,-5882,-2621,-1053,3281,432,4956,3604,1748,
  -5890,2749,1156,-1758,-5831,-3060,1762,3146,-4189,-5387,6130,-4092,-5981,5460,-2903,-2240,
  4342,4070,2315,3687,4407,378,-1808,-2991,-4615,4063,-2203,-5133,1534,-3566,3467,4614,
  5825,943,2967,-3853,2564,-625,-1367,-1004,-1960,727,489,-5655,1690,2320,3403,-1582,
  -1081,-2502,2019,5753,117,-1730,-48,-4647,965,-5656,-711,1060,1237,-3901,-2083,340,
  5266,-1715,-900,1964,-3412,4551,2030,-4703,1688,-2482,883,-5914,-5719,-4506,-4586,-42,
  -2530,-3764,-4949,914,-5217,-3526,1195,5858,-5847,-4609,-6109,5357,-4426,3159,2446,-1296,
  -2579,1477,-5244,5381,4042,-3468,5274,5204,-3109,-5286,2851,278,3872,-6101,-13,5654,
  -4091,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -5,-12804,-6719,-11551,-12367,-5344,15156,17316,25710,-5424,19700,-17940,-16484,5680,5082,12794,
  -18889,-1888,-10735,-19390,-25843,12996,8922,-8693,-30547,-8197,-19396,26888,6277,-2629,-21060,29646,
  6959,-2080,-12938,853,17071,26537,13162,12639,3002,-144,-18036,15183,15801,-6229,-13204,15999,
  8778,-26728,-15364,7605,-25625,13684,22276,6149,17748,14282,14564,-27704,747,23129,23219,-21252,
  27443,27203,-24702,603,5093,-27134,-5450,21214,13999,-8703,9381,-21636,23038,2154,-4410,27683,
  13572,15529,-3888,-28216,16719,-32115,28430,-28835,-26787,-25139,-773,-21614,8719,29032,-23742,11615,
  -30590,20441,-7615,16,-27123,20158,-30883,-28435,16031,20068,13588,-11594,16271,6437,-11716,-16084,
  -17039,-15247,27155,-8869,5664,32205,-7365,11994,26547,-26766,26078,26105,24590,-7349,-15129,-18830,
  7887,-2357,-23401,-20878,6239,-26723,-2560,14324,-14074,26051,27619,-9007,432,-11428,19988,18132,
  18686,-25923,17540,-26334,14649,-19444,-22814,11338,24483,-1291,-18446,12292,22691,21844,17577,-2240,
  -3850,-4122,-1781,-16793,-16073,8570,-1808,-15279,15865,16351,1893,23539,26110,-28142,-629,-3578,
  -6463,13231,-17513,24819,18948,11663,19113,15380,30808,-19753,20969,14825,9882,2320,-693,-26158,
  -21561,5690,30691,26233,4213,22846,-48,15833,5061,27112,19769,17444,5333,24771,-30755,16724,
  -19310,-30387,-17284,-14420,-19796,-15929,26606,-16991,-31080,22094,29555,-14106,14761,-12698,-12778,-8234,
  22046,-20148,-9045,-23662,7071,4666,-2901,-18718,14633,7679,22563,-23315,-12618,-17321,27022,-1296,
  -31251,5573,11140,9477,12234,12916,13466,21588,-7205,2906,31523,-7914,3872,-10197,28659,-2538,
  5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

};





static inline __m256i sub_x16(__m256i a,__m256i b)
{
  __asm__("vpsubw %1,%0,%0" : "+x"(a),"+x"(b));
  return a;
}

static inline __m256i add_x16(__m256i a,__m256i b)
{
  __asm__("vpaddw %1,%0,%0" : "+x"(a),"+x"(b));
  return a;
}

static inline __m256i reduce_x16(const __m256i *qdata,__m256i x)
{
  __m256i y = _mm256_mulhi_epi16(x,qrecip_x16);
  y = _mm256_mulhrs_epi16(y,qshift_x16);
  y = _mm256_mullo_epi16(y,q_x16);
  return sub_x16(x,y);
}

static inline __m256i mulmod_x16_scaled(const __m256i *qdata,__m256i x,__m256i y,__m256i yqinv)
{
  __m256i b = _mm256_mulhi_epi16(x,y);
  __m256i d = _mm256_mullo_epi16(x,yqinv);
  __m256i e = _mm256_mulhi_epi16(d,q_x16);
  return sub_x16(b,e);
}

static __attribute((aligned(32))) int8 shuffle_buf[32] = {
  14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,
  14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,
};
#define shuffle (*(__m256i *) shuffle_buf)

static inline __m256i _mm256_loadu_reverse16(const __m256i *p)
{
  __m256i x = _mm256_loadu_si256(p);
  x = _mm256_permute2x128_si256(x,x,1);
  x = _mm256_shuffle_epi8(x,shuffle);
  return x;
}

static void ntt128(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  for (rep = 0;rep < reps;++rep) {
    f1 = _mm256_loadu_si256((__m256i *) (f + 32));
    f3 = _mm256_loadu_si256((__m256i *) (f + 96));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 64));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv(128,0),zetainv_qinv(128,0));
    f2 = mulmod_x16_scaled(qdata,f2,zeta(128,0),zeta_qinv(128,0));
    
    g2 = _mm256_unpacklo_epi16(f2,f3);
    g3 = _mm256_unpackhi_epi16(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta(64,0),zeta_qinv(64,0));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = _mm256_unpacklo_epi16(f0,f1);
    h0 = _mm256_unpacklo_epi32(g0,g2);
    h1 = _mm256_unpackhi_epi32(g0,g2);
    g1 = _mm256_unpackhi_epi16(f0,f1);
    h2 = _mm256_unpacklo_epi32(g1,g3);
    h3 = _mm256_unpackhi_epi32(g1,g3);
    f1 = _mm256_permute2x128_si256(h2,h3,0x20);
    f3 = _mm256_permute2x128_si256(h2,h3,0x31);
    f0 = _mm256_permute2x128_si256(h0,h1,0x20);
    f2 = _mm256_permute2x128_si256(h0,h1,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 32),f1);
    _mm256_storeu_si256((__m256i *) (f + 64),f2);
    _mm256_storeu_si256((__m256i *) (f + 96),f3);
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 48));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 16));
    f2 = _mm256_loadu_si256((__m256i *) (f + 80));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv(128,1),zetainv_qinv(128,1));
    f2 = mulmod_x16_scaled(qdata,f2,zeta(128,1),zeta_qinv(128,1));
    
    g2 = _mm256_unpacklo_epi16(f2,f3);
    g3 = _mm256_unpackhi_epi16(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta(64,1),zeta_qinv(64,1));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = _mm256_unpacklo_epi16(f0,f1);
    h0 = _mm256_unpacklo_epi32(g0,g2);
    h1 = _mm256_unpackhi_epi32(g0,g2);
    g1 = _mm256_unpackhi_epi16(f0,f1);
    h2 = _mm256_unpacklo_epi32(g1,g3);
    h3 = _mm256_unpackhi_epi32(g1,g3);
    f1 = _mm256_permute2x128_si256(h2,h3,0x20);
    f3 = _mm256_permute2x128_si256(h2,h3,0x31);
    f0 = _mm256_permute2x128_si256(h0,h1,0x20);
    f2 = _mm256_permute2x128_si256(h0,h1,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 16),f0);
    _mm256_storeu_si256((__m256i *) (f + 48),f1);
    _mm256_storeu_si256((__m256i *) (f + 80),f2);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f1 = _mm256_loadu_si256((__m256i *) (f + 64));
    f3 = _mm256_loadu_si256((__m256i *) (f + 80));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 16));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_x4(32,0),zetainv_x4_qinv(32,0));
    f2 = mulmod_x16_scaled(qdata,f2,zeta_x4(32,0),zeta_x4_qinv(32,0));
    
    g2 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta_x4(16,0),zeta_x4_qinv(16,0));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g1 = _mm256_unpackhi_epi64(f0,f1);
    f1 = _mm256_permute2x128_si256(g1,g3,0x20);
    f3 = _mm256_permute2x128_si256(g1,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g2,0x20);
    f2 = _mm256_permute2x128_si256(g0,g2,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 64),f1);
    _mm256_storeu_si256((__m256i *) (f + 16),f2);
    _mm256_storeu_si256((__m256i *) (f + 80),f3);
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 96));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 32));
    f2 = _mm256_loadu_si256((__m256i *) (f + 48));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_x4(32,1),zetainv_x4_qinv(32,1));
    f2 = mulmod_x16_scaled(qdata,f2,zeta_x4(32,1),zeta_x4_qinv(32,1));
    
    g2 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta_x4(16,1),zeta_x4_qinv(16,1));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g1 = _mm256_unpackhi_epi64(f0,f1);
    f1 = _mm256_permute2x128_si256(g1,g3,0x20);
    f3 = _mm256_permute2x128_si256(g1,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g2,0x20);
    f2 = _mm256_permute2x128_si256(g0,g2,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 32),f0);
    _mm256_storeu_si256((__m256i *) (f + 96),f1);
    _mm256_storeu_si256((__m256i *) (f + 48),f2);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 16));
    f3 = _mm256_loadu_si256((__m256i *) (f + 48));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 32));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f2 = add_x16(g2,g3);
    f3 = sub_x16(g2,g3);
    f2 = reduce_x16(qdata,f2);
    f3 = reduce_x16(qdata,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    h0 = f0;
    h1 = f1;
    h2 = f2;
    h3 = f3;
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 80));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 64));
    f2 = _mm256_loadu_si256((__m256i *) (f + 96));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv8_x16,zetainv8_x16_qinv);
    f2 = mulmod_x16_scaled(qdata,f2,zeta8_x16,zeta8_x16_qinv);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta4_x16,zeta4_x16_qinv);
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = add_x16(h0,f0);
    g1 = add_x16(h1,f1);
    g2 = add_x16(h2,f2);
    g3 = add_x16(h3,f3);
    _mm256_storeu_si256((__m256i *) (f + 0),g0);
    _mm256_storeu_si256((__m256i *) (f + 16),g1);
    _mm256_storeu_si256((__m256i *) (f + 32),g2);
    _mm256_storeu_si256((__m256i *) (f + 48),g3);
    g0 = sub_x16(h0,f0);
    g1 = sub_x16(h1,f1);
    g2 = sub_x16(h2,f2);
    g3 = sub_x16(h3,f3);
    _mm256_storeu_si256((__m256i *) (f + 64),g0);
    _mm256_storeu_si256((__m256i *) (f + 80),g1);
    _mm256_storeu_si256((__m256i *) (f + 96),g2);
    _mm256_storeu_si256((__m256i *) (f + 112),g3);
    f += 128;
  }
}

static void invntt128(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f +   0));
    f2 = _mm256_loadu_si256((__m256i *) (f +  16));
    g0 = _mm256_loadu_si256((__m256i *) (f +  32));
    g2 = _mm256_loadu_si256((__m256i *) (f +  48));
    f1 = _mm256_loadu_si256((__m256i *) (f +  64));
    f3 = _mm256_loadu_si256((__m256i *) (f +  80));
    g1 = _mm256_loadu_si256((__m256i *) (f +  96));
    g3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    h0 = add_x16(f0,f1);
    h1 = sub_x16(f0,f1);
    h2 = add_x16(f2,f3);
    h3 = sub_x16(f2,f3);
    f0 = add_x16(g0,g1);
    f1 = sub_x16(g0,g1);
    f2 = add_x16(g2,g3);
    f3 = sub_x16(g2,g3);
    
    h0 = reduce_x16(qdata,h0);
    h1 = reduce_x16(qdata,h1);
    h2 = reduce_x16(qdata,h2);
    h3 = mulmod_x16_scaled(qdata,h3,zeta4_x16,zeta4_x16_qinv);
    f0 = reduce_x16(qdata,f0);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv8_x16,zetainv8_x16_qinv);
    f2 = reduce_x16(qdata,f2);
    f3 = mulmod_x16_scaled(qdata,f3,zeta8_x16,zeta8_x16_qinv);
    
    g0 = add_x16(h0,h2);
    g2 = sub_x16(h0,h2);
    g1 = sub_x16(h1,h3);
    g3 = add_x16(h1,h3);
    h0 = add_x16(f0,f2);
    h2 = sub_x16(f0,f2);
    h1 = sub_x16(f1,f3);
    h3 = add_x16(f1,f3);
    
    h2 = mulmod_x16_scaled(qdata,h2,zeta4_x16,zeta4_x16_qinv);
    h3 = mulmod_x16_scaled(qdata,h3,zeta4_x16,zeta4_x16_qinv);
    
    f0 = add_x16(g0,h0);
    g0 = sub_x16(g0,h0);
    f1 = add_x16(g1,h1);
    g1 = sub_x16(g1,h1);
    f2 = sub_x16(g2,h2);
    g2 = add_x16(g2,h2);
    f3 = sub_x16(g3,h3);
    g3 = add_x16(g3,h3);
    
    _mm256_storeu_si256((__m256i *) (f +   0),f0);
    _mm256_storeu_si256((__m256i *) (f +  16),f2);
    _mm256_storeu_si256((__m256i *) (f +  32),g0);
    _mm256_storeu_si256((__m256i *) (f +  48),g2);
    _mm256_storeu_si256((__m256i *) (f +  64),f1);
    _mm256_storeu_si256((__m256i *) (f +  80),f3);
    _mm256_storeu_si256((__m256i *) (f +  96),g1);
    _mm256_storeu_si256((__m256i *) (f + 112),g3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 16));
    f1 = _mm256_loadu_si256((__m256i *) (f + 64));
    f3 = _mm256_loadu_si256((__m256i *) (f + 80));
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g2 = _mm256_unpackhi_epi64(f0,f1);
    g1 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    f0 = _mm256_permute2x128_si256(g0,g1,0x20);
    f2 = _mm256_permute2x128_si256(g0,g1,0x31);
    f1 = _mm256_permute2x128_si256(g2,g3,0x20);
    f3 = _mm256_permute2x128_si256(g2,g3,0x31);
    
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_x4(16,0),zetainv_x4_qinv(16,0));
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_x4(32,0),zetainv_x4_qinv(32,0));
    f3 = mulmod_x16_scaled(qdata,f3,zeta_x4(32,0),zeta_x4_qinv(32,0));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = sub_x16(f0,f1);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 16),f2);
    _mm256_storeu_si256((__m256i *) (f + 64),f1);
    _mm256_storeu_si256((__m256i *) (f + 80),f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 32));
    f2 = _mm256_loadu_si256((__m256i *) (f + 48));
    f1 = _mm256_loadu_si256((__m256i *) (f + 96));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g2 = _mm256_unpackhi_epi64(f0,f1);
    g1 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    f0 = _mm256_permute2x128_si256(g0,g1,0x20);
    f2 = _mm256_permute2x128_si256(g0,g1,0x31);
    f1 = _mm256_permute2x128_si256(g2,g3,0x20);
    f3 = _mm256_permute2x128_si256(g2,g3,0x31);
    
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_x4(16,1),zetainv_x4_qinv(16,1));
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_x4(32,1),zetainv_x4_qinv(32,1));
    f3 = mulmod_x16_scaled(qdata,f3,zeta_x4(32,1),zeta_x4_qinv(32,1));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = sub_x16(f0,f1);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    
    _mm256_storeu_si256((__m256i *) (f + 32),f0);
    _mm256_storeu_si256((__m256i *) (f + 48),f2);
    _mm256_storeu_si256((__m256i *) (f + 96),f1);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f1 = _mm256_loadu_si256((__m256i *) (f + 32));
    f2 = _mm256_loadu_si256((__m256i *) (f + 64));
    f3 = _mm256_loadu_si256((__m256i *) (f + 96));
    
    g0 = _mm256_permute2x128_si256(f0,f2,0x20);
    g2 = _mm256_permute2x128_si256(f0,f2,0x31);
    g1 = _mm256_permute2x128_si256(f1,f3,0x20);
    g3 = _mm256_permute2x128_si256(f1,f3,0x31);
    f0 = _mm256_unpacklo_epi16(g0,g2);
    f2 = _mm256_unpackhi_epi16(g0,g2);
    f1 = _mm256_unpacklo_epi16(g1,g3);
    f3 = _mm256_unpackhi_epi16(g1,g3);
    g0 = _mm256_unpacklo_epi16(f0,f2);
    g1 = _mm256_unpackhi_epi16(f0,f2);
    g2 = _mm256_unpacklo_epi16(f1,f3);
    g3 = _mm256_unpackhi_epi16(f1,f3);
    f0 = _mm256_unpacklo_epi64(g0,g2);
    f1 = _mm256_unpackhi_epi64(g0,g2);
    f2 = _mm256_unpacklo_epi64(g1,g3);
    f3 = _mm256_unpackhi_epi64(g1,g3);
    
    f1 = mulmod_x16_scaled(qdata,f1,zetainv(64,0),zetainv_qinv(64,0));
    f2 = mulmod_x16_scaled(qdata,f2,zetainv(128,0),zetainv_qinv(128,0));
    f3 = mulmod_x16_scaled(qdata,f3,zeta(128,0),zeta_qinv(128,0));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = sub_x16(f0,f1);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 32),f1);
    _mm256_storeu_si256((__m256i *) (f + 64),f2);
    _mm256_storeu_si256((__m256i *) (f + 96),f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 16));
    f1 = _mm256_loadu_si256((__m256i *) (f + 48));
    f2 = _mm256_loadu_si256((__m256i *) (f + 80));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    g0 = _mm256_permute2x128_si256(f0,f2,0x20);
    g2 = _mm256_permute2x128_si256(f0,f2,0x31);
    g1 = _mm256_permute2x128_si256(f1,f3,0x20);
    g3 = _mm256_permute2x128_si256(f1,f3,0x31);
    f0 = _mm256_unpacklo_epi16(g0,g2);
    f2 = _mm256_unpackhi_epi16(g0,g2);
    f1 = _mm256_unpacklo_epi16(g1,g3);
    f3 = _mm256_unpackhi_epi16(g1,g3);
    g0 = _mm256_unpacklo_epi16(f0,f2);
    g1 = _mm256_unpackhi_epi16(f0,f2);
    g2 = _mm256_unpacklo_epi16(f1,f3);
    g3 = _mm256_unpackhi_epi16(f1,f3);
    f0 = _mm256_unpacklo_epi64(g0,g2);
    f1 = _mm256_unpackhi_epi64(g0,g2);
    f2 = _mm256_unpacklo_epi64(g1,g3);
    f3 = _mm256_unpackhi_epi64(g1,g3);
    
    f1 = mulmod_x16_scaled(qdata,f1,zetainv(64,1),zetainv_qinv(64,1));
    f2 = mulmod_x16_scaled(qdata,f2,zetainv(128,1),zetainv_qinv(128,1));
    f3 = mulmod_x16_scaled(qdata,f3,zeta(128,1),zeta_qinv(128,1));
    f0 = reduce_x16(qdata,f0);
    f1 = reduce_x16(qdata,f1);
    
    g0 = sub_x16(f0,f1);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    
    _mm256_storeu_si256((__m256i *) (f + 16),f0);
    _mm256_storeu_si256((__m256i *) (f + 48),f1);
    _mm256_storeu_si256((__m256i *) (f + 80),f2);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    
    f += 128;
  }
}

static void ntt512(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  for (rep = 0;rep < reps;++rep) {
    int i;
    
    for (i = 0;i < 8;++i) {
      f1 = _mm256_loadu_si256((__m256i *) (f + 16*i + 128));
      f3 = _mm256_loadu_si256((__m256i *) (f + 16*i + 384));
      g3 = sub_x16(f1,f3);
      g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
      g1 = add_x16(f1,f3);
      
      f0 = _mm256_loadu_si256((__m256i *) (f + 16*i));
      f2 = _mm256_loadu_si256((__m256i *) (f + 16*i + 256));
      g2 = sub_x16(f0,f2);
      g0 = add_x16(f0,f2);
      
      f3 = sub_x16(g3,g2);
      f2 = add_x16(g2,g3);
      f3 = mulmod_x16_scaled(qdata,f3,zetainv(512,i),zetainv_qinv(512,i));
      f2 = mulmod_x16_scaled(qdata,f2,zeta(512,i),zeta_qinv(512,i));
      
      f1 = sub_x16(g0,g1);
      f0 = add_x16(g0,g1);
      f1 = mulmod_x16_scaled(qdata,f1,zeta(256,i),zeta_qinv(256,i));
      f0 = reduce_x16(qdata,f0);
      f1 = reduce_x16(qdata,f1);
      
      _mm256_storeu_si256((__m256i *) (f + 16*i),f0);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 128),f1);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 256),f2);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 384),f3);
      
    }
    f += 512;
  }
  f = origf;
  ntt128(f,reps*4,qdata);
}

static void ntt512_256(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  for (rep = 0;rep < reps;++rep) {
    int i;
    
    for (i = 0;i < 8;++i) {
      f1 = _mm256_loadu_si256((__m256i *) (f + 16*i + 128));
      //f3 = _mm256_loadu_si256((__m256i *) (f + 16*i + 384));
      g3 = f1;
      g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
      g1 = f1;
      
      f0 = _mm256_loadu_si256((__m256i *) (f + 16*i));
      //f2 = _mm256_loadu_si256((__m256i *) (f + 16*i + 256));
      g2 = f0;
      g0 = f0;
      
      f3 = sub_x16(g3,g2);
      f2 = add_x16(g2,g3);
      f3 = mulmod_x16_scaled(qdata,f3,zetainv(512,i),zetainv_qinv(512,i));
      f2 = mulmod_x16_scaled(qdata,f2,zeta(512,i),zeta_qinv(512,i));
      
      f1 = sub_x16(g0,g1);
      f0 = add_x16(g0,g1);
      f1 = mulmod_x16_scaled(qdata,f1,zeta(256,i),zeta_qinv(256,i));
      f0 = reduce_x16(qdata,f0);
      f1 = reduce_x16(qdata,f1);
      
      _mm256_storeu_si256((__m256i *) (f + 16*i),f0);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 128),f1);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 256),f2);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 384),f3);
      
    }
    f += 512;
  }
  f = origf;
  ntt128(f,reps*4,qdata);
}

static void ntt512_7681(int16 *f,int reps)
{
  ntt512(f,reps,(const __m256i *) qdata_7681);
}

void ntt512_256_7681(int16 *f,int reps)
{
  ntt512_256(f,reps,(const __m256i *) qdata_7681);
}


static void ntt512_10753(int16 *f,int reps)
{
  ntt512(f,reps,(const __m256i *) qdata_10753);
}

void ntt512_256_10753(int16 *f,int reps)
{
  ntt512_256(f,reps,(const __m256i *) qdata_10753);
}


static void ntt512_12289(int16 *f,int reps)
{
  ntt512(f,reps,(const __m256i *) qdata_12289);
}

void ntt512_256_12289(int16 *f,int reps)
{
  ntt512_256(f,reps,(const __m256i *) qdata_12289);
}



static void invntt512(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  int i;
  invntt128(f,4*reps,qdata);
  for (rep = 0;rep < reps;++rep) {
    for(i = 0;i < 8;++i) {
      f0 = _mm256_loadu_si256((__m256i *) (f + 16*i + 0));
      f1 = _mm256_loadu_si256((__m256i *) (f + 16*i + 128));
      f2 = _mm256_loadu_si256((__m256i *) (f + 16*i + 256));
      f3 = _mm256_loadu_si256((__m256i *) (f + 16*i + 384));
      
      f1 = mulmod_x16_scaled(qdata,f1,zetainv(256,i),zetainv_qinv(256,i));
      f2 = mulmod_x16_scaled(qdata,f2,zetainv(512,i),zetainv_qinv(512,i));
      f3 = mulmod_x16_scaled(qdata,f3,zeta(512,i),zeta_qinv(512,i));
      f0 = reduce_x16(qdata,f0);
      f1 = reduce_x16(qdata,f1);
      
      g0 = sub_x16(f0,f1);
      g1 = add_x16(f0,f1);
      g2 = sub_x16(f3,f2);
      g3 = add_x16(f3,f2);
      g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
      
      f0 = add_x16(g0,g2);
      f2 = sub_x16(g0,g2);
      f1 = add_x16(g1,g3);
      f3 = sub_x16(g1,g3);
      
      _mm256_storeu_si256((__m256i *) (f + 16*i + 0),f0);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 128),f1);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 256),f2);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 384),f3);
    }
  f += 512;
  }
}

static
void avx3_invntt512(int16 *f,int reps, const __m256i *qdata)
{
  __m256i t;
  t = _mm256_loadu_si256((__m256i *) (f + 0));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 0),t);
  t = _mm256_loadu_si256((__m256i *) (f + 16));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 16),t);
  t = _mm256_loadu_si256((__m256i *) (f + 32));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 32),t);
  t = _mm256_loadu_si256((__m256i *) (f + 48));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 48),t);
  t = _mm256_loadu_si256((__m256i *) (f + 64));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 64),t);
  t = _mm256_loadu_si256((__m256i *) (f + 80));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 80),t);
  t = _mm256_loadu_si256((__m256i *) (f + 96));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 96),t);
  t = _mm256_loadu_si256((__m256i *) (f + 112));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 112),t);
  t = _mm256_loadu_si256((__m256i *) (f + 128));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 128),t);
  t = _mm256_loadu_si256((__m256i *) (f + 144));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 144),t);
  t = _mm256_loadu_si256((__m256i *) (f + 160));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 160),t);
  t = _mm256_loadu_si256((__m256i *) (f + 176));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 176),t);
  t = _mm256_loadu_si256((__m256i *) (f + 192));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 192),t);
  t = _mm256_loadu_si256((__m256i *) (f + 208));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 208),t);
  t = _mm256_loadu_si256((__m256i *) (f + 224));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 224),t);
  t = _mm256_loadu_si256((__m256i *) (f + 240));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 240),t);
  t = _mm256_loadu_si256((__m256i *) (f + 256));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 256),t);
  t = _mm256_loadu_si256((__m256i *) (f + 272));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 272),t);
  t = _mm256_loadu_si256((__m256i *) (f + 288));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 288),t);
  t = _mm256_loadu_si256((__m256i *) (f + 304));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 304),t);
  t = _mm256_loadu_si256((__m256i *) (f + 320));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 320),t);
  t = _mm256_loadu_si256((__m256i *) (f + 336));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 336),t);
  t = _mm256_loadu_si256((__m256i *) (f + 352));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 352),t);
  t = _mm256_loadu_si256((__m256i *) (f + 368));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 368),t);
  t = _mm256_loadu_si256((__m256i *) (f + 384));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 384),t);
  t = _mm256_loadu_si256((__m256i *) (f + 400));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 400),t);
  t = _mm256_loadu_si256((__m256i *) (f + 416));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 416),t);
  t = _mm256_loadu_si256((__m256i *) (f + 432));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 432),t);
  t = _mm256_loadu_si256((__m256i *) (f + 448));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 448),t);
  t = _mm256_loadu_si256((__m256i *) (f + 464));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 464),t);
  t = _mm256_loadu_si256((__m256i *) (f + 480));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 480),t);
  t = _mm256_loadu_si256((__m256i *) (f + 496));
  t = mulmod_x16_scaled(qdata,t,p2inv_512,p2inv_512_qinv);
  _mm256_storeu_si256((__m256i *) (f + 496),t);
  invntt512(f,reps,qdata);
}


void invntt512_7681(int16 *f,int reps)
{
  const __m256i *qdata = (const __m256i *) qdata_7681;
  avx3_invntt512(f,reps,qdata);
}

void invntt512_10753(int16 *f,int reps)
{
  const __m256i *qdata = (const __m256i *) qdata_10753;
  avx3_invntt512(f,reps,qdata);
}

void invntt512_12289(int16 *f,int reps)
{
  const __m256i *qdata = (const __m256i *) qdata_12289;
  avx3_invntt512(f,reps,qdata);
}
